select(UniqueCarrier, DepDelay) %>%
arrange(desc(DepDelay)) %>%
filter(UniqueCarrier %in% c('AS','YV'))
View(d7)
View(d7)
flights %>%
group_by(Dest) %>%
summarise(flight_count = n(),
plane_count = n_distinct(TailNum))
library(tidyr)
wide= data.frame(
name = c("Wilbur", "Petunia", "Gregory"),
a = c(67, 80, 64),
b = c(56, 90, 50),
c=c(10,20,30),
d=c(0,1,2),
e=c(-3,5,6)
)
wide
wide %>%
gather(drug,measurement,a:e)
wide= data.frame(
name = c("Wilbur", "Petunia", "Gregory"),
a = c(67, 80, 64),
b = c(56, 90, 50),
c=c(10,20,30),
d=c(0,1,2),
e=c(-3,5,6)
)
wide
wide %>%
gather(drug,measurement,a:e)
set.seed(10)
wide= data.frame(
id = 1:4,
trt = rep(c('control', 'treatment'), each = 2),
work.T1 = runif(4),
home.T1 = runif(4),
work.T2 = runif(4),
home.T2 = runif(4)
)
wide
long = wide %>%
gather(key,time,work.T1:home.T2)
long
flights %>%
group_by(Month) %>%
flights %>%
group_by(Month)
flights %>%
group_by(Month)
flights %>%
group_by(Month) %>%
summarise(flight_count = n())
flights %>%
group_by(Month, DayofMonth) %>%
summarise(flight_count = n()) %>%
arrange(desc(flight_count) )
flights %>%
group_by(Month) %>%
summarise(flight_count = n()) %>%
mutate(lagged_col=lag(flight_count,1),
change = flight_count - lagged_col)
flights
flights=as_tibble(hflights) #to get a better idea of the data
flights
library(dplyr)
library(hflights)
flights=as_tibble(hflights) #to get a better idea of the data
flights
flights %>%
group_by(Month)
d9= flights %>%
group_by(Month)
View(d9)
View(d9)
d9= flights %>%
group_by(Month) %>%
summarise(flight_count = n())
d9= flights %>%
group_by(Month) %>%
summarise(flight_count = n()) %>%
mutate(lagged_col=lag(flight_count,1)
d9= flights %>%
group_by(Month) %>%
summarise(flight_count = n()) %>%
mutate(lagged_col=lag(flight_count,1),
change = flight_count - lagged_col)
d9= flights %>%
group_by(Month) %>%
summarise(flight_count = n()) %>%
mutate(lagged_col=lag(flight_count,1),
change = flight_count - lagged_col)
flights %>%
select(Dest,DepDelay,ArrDelay,AirTime,Distance) %>%
group_by(Dest) %>%
summarise(depdelay_avg=mean(DepDelay,na.rm=T),
arrdelay_avg=mean(ArrDelay,na.rm=T),
airtime_avg=mean(AirTime,na.rm=T),
distance_avg=mean(Distance,na.rm=T))
flights %>%
select(Dest,DepDelay,ArrDelay,AirTime,Distance) %>%
group_by(Dest) %>%
summarise_all(mean,na.rm=T)
str(flights)
glimpse(flights)
d10= flights %>%
group_by(Dest, Month) %>%
summarise(Highest_flight_count = n())
View(d10)
View(d10)
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n() ,
max_month = max(monthly_flight_count))
d10= flights %>%
group_by(Dest, Month) %>%
summarise( max_month =max(n()))
d10= flights %>%
group_by(Dest, Month) %>%
mutate( max_month =max(n()))
d10= flights %>%
group_by(Dest, Month) %>%
summarise( max_month =max(n()))
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n())
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n(),
monthly_distinct_flight = n_distinct())
d10= flights %>%
group_by(Dest, Month) %>%
summarise(max(monthly_flight_count = n()) )
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (monthly_flight_count)
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (desc(monthly_flight_count))
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (desc(monthly_flight_count)) %>%
filter (max(monthly_flight_count))
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
#arrange (desc(monthly_flight_count)) %>%
filter (max(monthly_flight_count))
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (desc(monthly_flight_count))
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (Dest,desc(monthly_flight_count))
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (Dest,desc(monthly_flight_count)) %>%
sort(flights)
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (Dest,desc(monthly_flight_count)) %>%
sort(flights,partial=length(flights)-1)[n-1]
d10= flights %>%
group_by(Dest, Month) %>%
summarise(monthly_flight_count = n()) %>%
arrange (Dest,desc(monthly_flight_count)) %>%
sort(partial=length(flights)-1)[n-1]
wq=read.csv("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\winequality-white.csv",
sep=";")
t.test(wq$alcohol,mu = 10)
t.test(wq$alcohol,mu = 10.5)
t.test(wq$alcohol,mu = 11)
## ------------------------------------------------------------------------
t.test(wq$alcohol,mu = 10,alternative ="less" )
t.test(wq$alcohol,mu = 10,alternative ="greater" )
t.test(wq$alcohol,mu = 11,alternative ="greater" )
t.test(wq$alcohol,mu = 10)
t.test(wq$alcohol,mu = 10.6)
t.test(wq$alcohol,mu = 10.5)
t.test(wq$alcohol,mu = 10.51)
mean(wq$alcohol)
## ----
# please install package sas7bdat
library(sas7bdat)
d=read.sas7bdat("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\hsb2.sas7bdat")
View(d)
View(d)
t.test(d$read,d$write,paired = TRUE)
t.test(wq$alcohol,mu = 10.5)
t.test(wq$alcohol,mu = 10.4)
t.test(wq$alcohol,mu = 10.6)
t.test(d$read,d$write,paired = TRUE)
mean(d$read)-mean(d$write)
read_f=d$read[d$female==1]
read_m=d$read[d$female==0]
var.test(read_f,read_m) # H_0 : ratio of variance = 1
t.test(read_f,read_m,paired = FALSE,var.equal = TRUE)
n
## ----
fit = aov(alcohol ~ quality ,data=wq)
d=read.sas7bdat("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\hsb2.sas7bdat")
## ----
# please install package sas7bdat
library(sas7bdat)
d=read.sas7bdat("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\hsb2.sas7bdat")
t.test(d$read,d$write,paired = TRUE)
## ----
fit = aov(alcohol ~ quality ,data=wq)
wq=read.csv("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\winequality-white.csv",
sep=";")
## ----
fit = aov(alcohol ~ quality ,data=wq)
summary(fit)
wq=read.csv("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\winequality-white.csv",
sep=";")
t.test(wq$alcohol,mu = 10.6)
## ------------------------------------------------------------------------
t.test(wq$alcohol,mu = 10,alternative ="less" )
t.test(wq$alcohol,mu = 11,alternative ="greater" )
## ----
# please install package sas7bdat
library(sas7bdat)
d=read.sas7bdat("D:\\IITK Data Analytics\\R\\Hypothesis-Testing-in-R-\\hsb2.sas7bdat")
t.test(d$read,d$write,paired = TRUE)
mean(d$read)-mean(d$write)
read_f=d$read[d$female==1]
read_m=d$read[d$female==0]
var.test(read_f,read_m) # H_0 : ratio of variance = 1
t.test(read_f,read_m,paired = FALSE,var.equal = TRUE)
## ----
fit = aov(alcohol ~ quality ,data=wq)
summary(fit)
fit
summary(fit)
summary(fit)
pairwise.t.test(wq$alcohol, wq$quality)
table(d$race)
prop.table(table(d$race))
View(wq)
View(wq)
min(wq$alcohol)
max(wq$alcohol)
## ------------------------------------------------------------------------
chisq.test(table(d$race),p=c(0.2,0.1,0.1,0.6))
## ------------------------------------------------------------------------
chisq.test(table(d$race),p=c(0.1,0.1,0.1,0.7))
## ------------------------------------------------------------------------
chisq.test(table(d$race),p=c(0.1,0.1,0.1,0.7))
## ------------------------------------------------------------------------
chisq.test(table(d$ses,d$female))
## ------------------------------------------------------------------------
table(d$race,d$ses)
## ------------------------------------------------------------------------
chisq.test(table(d$ses,d$race))
## ------------------------------------------------------------------------
fisher.test(table(d$race,d$ses))
# Predictive modeling : linear regression
#importing necessary libraries
library(tidymodels)
library(visdat)
library(tidyr)
library(car)
#reading out dataset
setwd("D:\\IITK Data Analytics\\R\\LinearRegression-with-tidymodels-R\\")
df_train = read.csv("loan_data_train (1).csv",stringsAsFactors = F)
df_test = read.csv("loan_data_test (1).csv",stringsAsFactors = F)
#studying the dataset
glimpse(df_train)
vis_dat(df_train)
#we see that we need to change columns to numeric type
###Data Preparation starts here
#Interest Rate(custom): remove % and change to numeric (custom fn)
#Debt.to.income ratio (custom): remove % and change to numeric
#FICO(custom): custom function and conv to numeric ()
# Employment length(custom): string cleaning & conv to numeric
#ID(drop) : drop this
#Amount.Requested(num): convert to numeric
#Amount.Funded.by.investors(drop): drop this
#Loan.Length(duuumy) :  we can use this as numeric feature or as a category.
#lets use this as a category and create dummies
#Loan.Purpose (dummy): club low freq together & create dummies
#state (dummies) :  club low freq together & create dummies
#Homeownership (dummy) : club low freq together & create dummies
# Monthly.Income: already numeric, impute missing values
# OPEN CREDIT LINES, revolving credit lines(num):  convert to numeric, impute missing values
#inquiries in last 6 months: do nothing, impute missing values
####
# we need to make 3 custom function
fico_fn=function(x){
temp=data.frame(fico=x)
temp=temp %>%
separate(fico,into = c('f1','f2')) %>%
mutate(f1=as.numeric(f1),
f2=as.numeric(f2),
fico = 0.5*(f1+f2) ) %>%
select(-f1,-f2)
return(temp[,'fico'])
}
emp_len_fn=function(x){
x=ifelse(x=="< 1 year",0,x)
x=gsub("years","",x)
x=gsub("year","",x)
x=gsub("+","",x,fixed=T) #without fixed=T, + has some special meaning
x=as.numeric(x)
return(x)
}
percent_to_numeric_fn=function(x){
x=gsub("%","",x)
x=as.numeric(x)
return(x)
}
###
#now we will use functions recipe(),prep() & bake() of library tidymodels
#this will make our data preprocessing code more......tidy
#run the code stepbystep to understand
dp_pipe = recipe(Interest.Rate ~ .,data=df_train) %>%
# update role of 'to-drop' columns first
update_role(ID,Amount.Funded.By.Investors,new_role = "drop_vars") %>%
# update role of to convert to numeric columns
update_role(Amount.Requested,
Open.CREDIT.Lines,
Revolving.CREDIT.Balance,new_role="to_numeric") %>%
update_role(Home.Ownership,State,Loan.Length,Loan.Purpose,new_role="to_dummies") %>%
#now we drop 'drop_vars'
step_rm(has_role('drop_vars')) %>%
#applying the custom functions
step_mutate_at(FICO.Range,fn=fico_fn) %>%
step_mutate_at(Employment.Length,fn=emp_len_fn) %>%
step_mutate_at(Debt.To.Income.Ratio,fn=percent_to_numeric_fn) %>%
step_mutate_at(Interest.Rate,fn=percent_to_numeric_fn,skip=TRUE) %>% #only for ld_train
step_mutate_at(has_role("to_numeric"),fn=as.numeric) %>%
# creating n-1 dummies for categoricl variables
#???????????????????????????????????????????????????????????????????
step_unknown(has_role("to_dummies"),new_level="__missing__") %>%
#cleaning & grouping them into __other__ . choice of threshold is subjective
step_other(has_role("to_dummies"),threshold =0.02,other="__other__") %>%
#create dummies
step_dummy(has_role("to_dummies")) %>% #creates n-1 dummies
#imputing values
step_impute_median(all_numeric(),-all_outcomes())
dp_pipe=prep(dp_pipe)
train=bake(dp_pipe,new_data = NULL)
test=bake(dp_pipe,new_data=df_test)
vis_dat(train) # lines show the NAs..
vis_dat(df_train)
library(visdat)
library(tidyr)
library(car)
#reading out dataset
setwd("D:\\IITK Data Analytics\\R\\LinearRegression-with-tidymodels-R\\")
df_train = read.csv("loan_data_train (1).csv",stringsAsFactors = F)
df_test = read.csv("loan_data_test (1).csv",stringsAsFactors = F)
#studying the dataset
glimpse(df_train)
vis_dat(df_train)
fico_fn=function(x){
temp=data.frame(fico=x)
temp=temp %>%
separate(fico,into = c('f1','f2')) %>%
mutate(f1=as.numeric(f1),
f2=as.numeric(f2),
fico = 0.5*(f1+f2) ) %>%
select(-f1,-f2)
return(temp[,'fico'])
}
emp_len_fn=function(x){
x=ifelse(x=="< 1 year",0,x)
x=gsub("years","",x)
x=gsub("year","",x)
x=gsub("+","",x,fixed=T) #without fixed=T, + has some special meaning
x=as.numeric(x)
return(x)
}
percent_to_numeric_fn=function(x){
x=gsub("%","",x)
x=as.numeric(x)
return(x)
}
#run the code stepbystep to understand
dp_pipe = recipe(Interest.Rate ~ .,data=df_train) %>%
# update role of 'to-drop' columns first
update_role(ID,Amount.Funded.By.Investors,new_role = "drop_vars") %>%
# update role of to convert to numeric columns
update_role(Amount.Requested,
Open.CREDIT.Lines,
Revolving.CREDIT.Balance,new_role="to_numeric") %>%
update_role(Home.Ownership,State,Loan.Length,Loan.Purpose,new_role="to_dummies") %>%
#now we drop 'drop_vars'
step_rm(has_role('drop_vars')) %>%
#applying the custom functions
step_mutate_at(FICO.Range,fn=fico_fn) %>%
step_mutate_at(Employment.Length,fn=emp_len_fn) %>%
step_mutate_at(Debt.To.Income.Ratio,fn=percent_to_numeric_fn) %>%
step_mutate_at(Interest.Rate,fn=percent_to_numeric_fn,skip=TRUE) %>% #only for ld_train
step_mutate_at(has_role("to_numeric"),fn=as.numeric) %>%
# creating n-1 dummies for categoricl variables
#???????????????????????????????????????????????????????????????????
step_unknown(has_role("to_dummies"),new_level="__missing__") %>%
#cleaning & grouping them into __other__ . choice of threshold is subjective
step_other(has_role("to_dummies"),threshold =0.02,other="__other__") %>%
#create dummies
step_dummy(has_role("to_dummies")) %>% #creates n-1 dummies
#imputing values
step_impute_median(all_numeric(),-all_outcomes())
dp_pipe=prep(dp_pipe)
train=bake(dp_pipe,new_data = NULL)
test=bake(dp_pipe,new_data=df_test)
vis_dat(train) # lines show the NAs..
vis_dat(df_train)
#first we split the train dataset into 80%
set.seed(2)
s=sample(1:nrow(train),0.8*nrow(train))
s
t1=train[s,]
t2=train[-s,]
?lm
fit=lm(Interest.Rate~.,data=t1)
fit
summary(fit)
#we take vif cutoff 5
sort(vif(fit),decreasing = T)
?vif
fit=lm(Interest.Rate~.-Loan.Purpose_debt_consolidation,data=t1)
sort(vif(fit),decreasing = T)
fit=lm(Interest.Rate~.-Loan.Purpose_debt_consolidation-State_X__other__,data=t1)
sort(vif(fit),decreasing = T)
summary(fit)
?stats::step
fit=stats::step(fit)
summary(fit)
formula(fit)
fit=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Inquiries.in.the.Last.6.Months +
Loan.Length_X60.months + Loan.Purpose_credit_card + Loan.Purpose_small_business +
State_IL + State_NC + State_NJ + State_TX + Home.Ownership_OWN +
Home.Ownership_RENT + Home.Ownership_X__other__,data=t1)
summary(fit)
t2.pred=predict(fit,newdata=t2)
errors = t2$Interest.Rate-t2.pred
rmse=errors**2 %>% mean() %>% sqrt()
mae=mean(abs(errors))
### Now we will do final prediction on the test set
fit.final=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Revolving.CREDIT.Balance +
Inquiries.in.the.Last.6.Months + Loan.Length_X60.months +
Loan.Purpose_credit_card + Loan.Purpose_major_purchase +
State_NC  + State_TX + Home.Ownership_OWN +
Home.Ownership_RENT + Home.Ownership_X__other__,
data=train)
sort(vif(fit.final),decreasing = T)
### Now we will do final prediction on the test set
fit.final=lm(Interest.Rate ~ .-Loan.Purpose_debt_consolidation
-State_X__other__,data=train)
sort(vif(fit.final),decreasing = T)
fit.final=stats::step(fit.final)
summary(fit.final)
fit.final=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Revolving.CREDIT.Balance +
Inquiries.in.the.Last.6.Months + Loan.Length_X60.months +
Loan.Purpose_credit_card + Loan.Purpose_major_purchase +
State_NC  + State_TX + Home.Ownership_OWN +
Home.Ownership_RENT + Home.Ownership_X__other__,
data=train)
fit.final=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Revolving.CREDIT.Balance +
Inquiries.in.the.Last.6.Months + Loan.Length_X60.months +
Loan.Purpose_credit_card + Loan.Purpose_major_purchase +
State_NC  + State_TX + Home.Ownership_OWN +
Home.Ownership_RENT + Home.Ownership_X__other__,
summary(fit.final)
test.pred=predict(fit.final,newdata=test)   data=train)
summary(fit.final)
test.pred=predict(fit.final,newdata=test)
write.csv(test.pred,"submision1.csv",row.names = F)
plot(fit.final,1) # residual vs fitted values => non-linearity in the data exists or not
plot(fit.final,2) # errors are normal or not
plot(fit.final,3) # variance is constant or not
plot(fit.final,4) # outliers in the data if cook's distance >1
_
#now manually drop those features whose pvalue is greater after the above step process
fit=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Inquiries.in.the.Last.6.Months +
Loan.Length_X60.months +  Loan.Purpose_small_business +
State_IL + State_NC +  State_TX  +
Home.Ownership_RENT + Home.Ownership_X__other__,data=t1)
summary(fit)
#now manually drop those features whose pvalue is greater after the above step process
fit=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Inquiries.in.the.Last.6.Months +
Loan.Length_X60.months +  Loan.Purpose_small_business +
State_IL +  State_TX  +
Home.Ownership_RENT + Home.Ownership_X__other__,data=t1)
summary(fit)
#now manually drop those features whose pvalue is greater after the above step process
fit=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Inquiries.in.the.Last.6.Months +
Loan.Length_X60.months +  Loan.Purpose_small_business +
State_IL +  State_TX   + Home.Ownership_X__other__,data=t1)
summary(fit)
fit.final=lm(Interest.Rate ~ .-Loan.Purpose_debt_consolidation
-State_X__other__,data=train)
sort(vif(fit.final),decreasing = T)
fit.final=stats::step(fit.final)
summary(fit.final)
fit.final=lm(Interest.Rate ~ Monthly.Income + FICO.Range + Revolving.CREDIT.Balance +
Inquiries.in.the.Last.6.Months + Loan.Length_X60.months +
Loan.Purpose_credit_card + Loan.Purpose_major_purchase +
State_NC  + State_TX + Home.Ownership_OWN +
Home.Ownership_RENT + Home.Ownership_X__other__,
data=train)
summary(fit.final)
